# Usage (to create or update):
# 
# gcloud init
# gcloud auth login
# gcloud components install kubectl
# gcloud container clusters get-credentials [cluster-name] --project [Project ID]
## (http://kubernetes.io/docs/user-guide/managing-deployments/#kubectl-apply)
# kubectl apply -f .
#
#
#
# inspired from
# https://github.com/kubernetes/kubernetes/blob/d4251b2a259ad3b185e13e218b13634d627e0274/examples/guestbook/all-in-one/guestbook-all-in-one.yaml
apiVersion: v1
kind: Service
metadata:
  name: thumbor-service
  annotations:
    networking.gke.io/load-balancer-type: "Internal"
    networking.gke.io/internal-load-balancer-allow-global-access: "true"
  labels:
    app: thumbor
    tier: backend
    role: master
spec:
  ports:
    # the port that this service should serve on
  - port: 28765
    targetPort: 80
  type: LoadBalancer
  selector:
    app: thumbor
    tier: backend
    role: master
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thumbor-deployment
spec:
  selector:
    matchLabels:
      app: thumbor
  replicas: 3
  template:
    metadata:
      labels:
        app: thumbor
        role: master
        tier: backend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: ["thumbor"]
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: master
        image: minimalcompact/thumbor:6.7.5
        # envvars
        env:
        - name: THUMBOR_NUM_PROCESSES # Normally this is set in connection with the number of CPU cores
          value: "2"
        - name: MAX_SOURCE_SIZE
          value: "10485760"
        - name: STORAGE_EXPIRATION_SECONDS
          value: "0"
        - name: DETECTORS
          value: "['thumbor.detectors.feature_detector','thumbor.detectors.face_detector']"
        - name: USE_GIFSICLE_ENGINE
          value: "True"
        # Increase this (& CPU limit accordingly) if you are seeing your IOLoop getting blocked,
        # often indicated by your upstream HTTP requests timing out
        - name: ENGINE_THREADPOOL_SIZE
          value: "4" # 2 x nproc
        - name: GC_INTERVAL
          value: "60" # in seconds
        - name: HTTP_LOADER_FOLLOW_REDIRECTS
          value: "False"
        - name: HTTP_LOADER_MAX_CLIENTS
          value: "25"
        - name: RESPECT_ORIENTATION
          value: "True"
        resources:
          requests:
            cpu: 1250m
            memory: 5120Mi
          limits:
            cpu: 2000m
            # do not set memory limit, pods are OOMkilled (even more, argh)
            # upon memory usage spikes
        livenessProbe:
          # an http probe
          httpGet:
            path: /healthcheck
            port: 80
          # length of time to wait for a pod to initialize
          # after pod startup, before applying health checking
          initialDelaySeconds: 45
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
          periodSeconds: 30
        readinessProbe:
          # an http probe
          httpGet:
            path: /healthcheck
            port: 80
          # length of time to wait for a pod to initialize
          # after pod startup, before applying health checking
          initialDelaySeconds: 20
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
          periodSeconds: 10
        ports:
        - containerPort: 80
---
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: thumbor
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: thumbor-deployment
  minReplicas: 3
  maxReplicas: 9
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleDown: # scale down max 1 pod every 1200 + 60 seconds
      stabilizationWindowSeconds: 1200 # wait 20 min for the highest recommendation
      policies: # limit scale down rate
      - type: Pods
        value: 1
        periodSeconds: 60
    scaleUp: # scale up max 1 pods every 600 + 60 seconds
      stabilizationWindowSeconds: 600 # wait 10 min for the lowest recommendation
      policies: # limit scale up rate
      - type: Pods
        value: 1
        periodSeconds: 60
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: thumbor
spec:
  minAvailable: 75%
  selector:
    matchLabels:
      app: thumbor
